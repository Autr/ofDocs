{"data":[{"page":{"entry":{"name":"README","filename":"README.md","absolute":"/Users/Gilbert/Code/openFrameworks/examples/computer_vision/opencvExample/README.md","path":"/examples/computer_vision/opencvExample/README","route":"/examples/computer_vision/opencvExample/README.md","dir":"../examples/computer_vision/opencvExample","ext":"md","type":"page","id":807,"parent":805,"breadcrumbs":[805,801,514],"translations":{},"siblings":[]},"description":{},"document":"<h1>About opencvExample</h1>\n<p><img src=\"opencvExample.png\" alt=\"Screenshot of opencvExample\"></p>\n<h3>Learning Objectives</h3>\n<p>OpenCV is a powerful open-source library for image processing and computer vision. This example demonstrates one particularly common workflow in new-media art: performing <em>background subtraction</em>, <em>blob detection</em> and <em>contour tracing</em>. This is immensely useful for locating objects or people that have entered a scene!</p>\n<p>After studying this example, you'll understand how to:</p>\n<ul>\n<li>Obtain video from a camera or stored file</li>\n<li>Use that video as the basis for image processing operations with OpenCV, including image arithmetic</li>\n<li>Extract blobs and their contours from background subtraction, a common workflow in computer vision applications.</li>\n</ul>\n<h3>Expected Behavior</h3>\n<p>When launching this app, you'll see a window that displays five different stages in processing a video.</p>\n<ol>\n<li>In the upper-left is the raw, unmodified video of a hand creating a shadow. Although it's not obvious, this is a color video (that happens to be showing a mostly black-and-white scene). After reading the hand video from its source file (using an <code>ofVideoPlayer</code>), this video is stored in <code>colorImg</code>, an <code>ofxCvColorImage</code>.</li>\n<li>In the upper-right of the display is the same video, converted to grayscale. Here it is stored in the <code>grayImage</code> object, which is an instance of an <code>ofxCvGrayscaleImage</code>. It's easy to miss the grayscale conversion; it's done implicitly in the assignment <code>grayImage = colorImg;</code> (line 48 in the ofApp.cpp file) using operator overloading of the <code>=</code> sign. In this example, all of the subsequent image processing is done with grayscale (rather than color) images.</li>\n<li>In the middle-left is a view of the <em>background image</em>. This is a grayscale image of the scene captured when the video first started playing, before the hand entered the frame. (See line 48 of the ofApp.cpp file.)</li>\n<li>In the middle-right is an image that shows the <em>thresholded absolute difference</em> between the current frame and the background frame. This image has been <em>binarized</em>, meaning that pixel values are either black (0) or white (255). The white pixels represent regions that are significantly different from the background: the hand!</li>\n<li>In the bottom right, an <code>ofxCvContourFinder</code> has been tasked to <code>findContours()</code> in the binarized image. It does this by identifying blobs of white pixels that meet certain area requirements -- and then tracing the contours of those blobs into an <code>ofxCvBlob</code> outline of (x,y) points. The app shows the contour of each blob in cyan, and also shows the bounding rectangle of those points in magenta. <em>Note:</em> The contour is a vector-based representation, and can be used for all sorts of further geometric play....</li>\n</ol>\n<p>There are a few user-modifiable settings in this app:</p>\n<ul>\n<li>Pressing the <code>space</code> bar will capture a fresh image of the background.</li>\n<li>Pressing the <code>+</code> and <code>-</code> keys will adjust the threshold used in the absolute differencing operation. The greater the threshold value, the more different a pixel needs to be from the background in order to be considered part of the &quot;foreground&quot;.</li>\n</ul>\n<p><strong>One more thing.</strong> In line 7 of the <code>ofApp.h</code> file, you'll see the following line commented out:</p>\n<pre><code class=\"language-cpp\"><span class=\"token comment\" spellcheck=\"true\">//#define _USE_LIVE_VIDEO</span>\n</code></pre>\n<p>If you uncomment this line, the app will use your computer's built-in webcam instead of a stored video file! It accomplishes this by swapping out the <code>ofVideoPlayer</code> with an <code>ofVideoGrabber</code>.</p>\n<h3>Other classes used in this file</h3>\n<p>This example links against the <code>ofxOpenCv</code> core addon. It uses the following classes from that addon:</p>\n<ul>\n<li><a href=\"http://openframeworks.cc/documentation/ofxOpenCv/ofxCvColorImage/\">ofxCvColorImage</a></li>\n<li><a href=\"http://openframeworks.cc/documentation/ofxOpenCv/ofxCvGrayscaleImage/\">ofxCvGrayscaleImage</a></li>\n<li><a href=\"http://openframeworks.cc/documentation/ofxOpenCv/ofxCvContourFinder/\">ofxCvContourFinder</a></li>\n<li><a href=\"http://openframeworks.cc/documentation/ofxOpenCv/ofxCvBlob/\">ofxCvBlob</a> <em>(used implicitly)</em></li>\n</ul>\n<p>In addition, this example uses the following classes to access  video from a live camera and/or a pre-stored file:</p>\n<ul>\n<li><a href=\"http://openframeworks.cc/documentation/video/ofVideoPlayer/\">ofVideoPlayer</a></li>\n<li><a href=\"http://openframeworks.cc/documentation/video/ofVideoGrabber/\">ofVideoGrabber</a></li>\n</ul>\n","raw":"# About opencvExample\n\n![Screenshot of opencvExample](opencvExample.png)\n\n### Learning Objectives\n\nOpenCV is a powerful open-source library for image processing and computer vision. This example demonstrates one particularly common workflow in new-media art: performing *background subtraction*, *blob detection* and *contour tracing*. This is immensely useful for locating objects or people that have entered a scene!\n\nAfter studying this example, you'll understand how to:\n\n* Obtain video from a camera or stored file\n* Use that video as the basis for image processing operations with OpenCV, including image arithmetic\n* Extract blobs and their contours from background subtraction, a common workflow in computer vision applications.\n\n\n### Expected Behavior\n\nWhen launching this app, you'll see a window that displays five different stages in processing a video.\n\n1. In the upper-left is the raw, unmodified video of a hand creating a shadow. Although it's not obvious, this is a color video (that happens to be showing a mostly black-and-white scene). After reading the hand video from its source file (using an `ofVideoPlayer`), this video is stored in `colorImg`, an `ofxCvColorImage`.\n2. In the upper-right of the display is the same video, converted to grayscale. Here it is stored in the `grayImage` object, which is an instance of an `ofxCvGrayscaleImage`. It's easy to miss the grayscale conversion; it's done implicitly in the assignment `grayImage = colorImg;` (line 48 in the ofApp.cpp file) using operator overloading of the `=` sign. In this example, all of the subsequent image processing is done with grayscale (rather than color) images.\n3. In the middle-left is a view of the *background image*. This is a grayscale image of the scene captured when the video first started playing, before the hand entered the frame. (See line 48 of the ofApp.cpp file.)\n4. In the middle-right is an image that shows the *thresholded absolute difference* between the current frame and the background frame. This image has been *binarized*, meaning that pixel values are either black (0) or white (255). The white pixels represent regions that are significantly different from the background: the hand!\n5. In the bottom right, an `ofxCvContourFinder` has been tasked to `findContours()` in the binarized image. It does this by identifying blobs of white pixels that meet certain area requirements -- and then tracing the contours of those blobs into an `ofxCvBlob` outline of (x,y) points. The app shows the contour of each blob in cyan, and also shows the bounding rectangle of those points in magenta. *Note:* The contour is a vector-based representation, and can be used for all sorts of further geometric play....\n\nThere are a few user-modifiable settings in this app:\n\n* Pressing the `space` bar will capture a fresh image of the background.\n* Pressing the `+` and `-` keys will adjust the threshold used in the absolute differencing operation. The greater the threshold value, the more different a pixel needs to be from the background in order to be considered part of the \"foreground\".\n\n**One more thing.** In line 7 of the `ofApp.h` file, you'll see the following line commented out:\n\n```cpp\n//#define _USE_LIVE_VIDEO\n```\n If you uncomment this line, the app will use your computer's built-in webcam instead of a stored video file! It accomplishes this by swapping out the `ofVideoPlayer` with an `ofVideoGrabber`.\n\n\n### Other classes used in this file\n\nThis example links against the `ofxOpenCv` core addon. It uses the following classes from that addon:\n\n* [ofxCvColorImage](http://openframeworks.cc/documentation/ofxOpenCv/ofxCvColorImage/)\n* [ofxCvGrayscaleImage](http://openframeworks.cc/documentation/ofxOpenCv/ofxCvGrayscaleImage/)\n* [ofxCvContourFinder](http://openframeworks.cc/documentation/ofxOpenCv/ofxCvContourFinder/)\n* [ofxCvBlob](http://openframeworks.cc/documentation/ofxOpenCv/ofxCvBlob/) *(used implicitly)*\n\nIn addition, this example uses the following classes to access  video from a live camera and/or a pre-stored file:\n\n* [ofVideoPlayer](http://openframeworks.cc/documentation/video/ofVideoPlayer/)\n* [ofVideoGrabber](http://openframeworks.cc/documentation/video/ofVideoGrabber/)\n","type":"page","static":true}}],"fetch":[]}